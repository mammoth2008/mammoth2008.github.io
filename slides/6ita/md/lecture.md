Tom Mitchell (Definition of the general learning problem):
A computer program is said to learn from experience $E$ with respect to some class of tasks $T$ and performance measure $P$, if its performance a tasks in $T$, as measured by $P$, improves with experience $E$

一个计算机程序在完成任务$T$之后,获得经验$E$,其表现效果为$P$;如果任务$T$的性能表现,也就是用以衡量的$P$,随着$E$的增加而增加,可以称其为学习

Mitchell T M. Machine learning[J]. Burr Ridge, IL: McGraw Hill, 1997, 45(37): 870-877.

计算机能够做到的只有两件事:计算和存储.

智能大概可以归纳为两种行为:搜索和评估.

搜索≈仿真

搜索+评估≈模拟

对于计算机来说,处理人类觉得复杂的事情很容易,处理人类觉得简单的事情很复杂.例如,如何看?如何说话?如何行走?

人工智能发展的历史:几个寒冬时期

增加神经元的层数,层数越多,学习的难度越大.当达到5层以上,就没有办法再学习了.直到反向传播的出现.

防止过拟合的一种方式:删除一些神经元:神经元脱离.

与机器学习中的其他技术相比,深度学习的优势在于,输入输出有更高的自由度.输入声音,输出图像;输入图像,输出文字,都可以.多模态

深度学习最擅长的,是图像处理.这也许和神经元这种结构有关,和大脑的结构有关.

传统上,我们解决问题遵循还原主义的思路:将整体分解,若可以分别理解各个部分的构造,就可以进一步理解整体;例如钟表.但是深度学习更像是涌现,这是复杂系统的一个特征,例如蚁群和蜂群.我们不能用还原主义的思路去理解它.

人类无法直观地理解指数级成长.人类总是倾向线性增长.棋盘的例子,一片池塘被睡莲覆盖的例子.

Monte Carlo Method

蒙特卡罗方法是一种计算方法.原理是通过大量随机样本,去了解一个系统,进而得到所要计算的值.20世纪40年代,在科学家冯·诺伊曼、斯塔尼斯拉夫·乌拉姆和尼古拉斯·梅特罗波利斯于洛斯阿拉莫斯国家实验室为核武器计划工作时,发明了蒙特卡罗方法.蒙特卡罗方法是以概率为基础的方法.

它非常强大和灵活,又相当简单易懂,很容易实现.对于许多问题来说,它往往是最简单的计算方法,有时甚至是唯一可行的方法.基本思想是：为了求解问题,首先建立一个概率模型或随机过程,使它的参数或数字特征等于问题的解：然后通过对模型或过程的观察或抽样试验来计算这些参数或数字特征,最后给出所求解的近似值.解的精确度用估计值的标准误差来表示.蒙特卡罗法的主要理论基础是概率统计理论,主要手段是随机抽样、统计试验.

贪心算法 Greedy Algorithm

又称贪婪算法,是一种在每一步选择中都采取在当前状态下最好或最优（即最有利）的选择,从而希望导致结果是最好或最优的算法. 比如在旅行推销员问题中,如果旅行员每次都选择最近的城市,那这就是一种贪心算法.

贝叶斯定理,朴素贝叶斯分类,基于统计的方法

if-else分类,本质上是基于二叉树的.

专家系统是基于规则的推理机
目前很多分析结果呈现系统属于专家系统

推荐引擎是一种预测缺失信息并将其推荐给用户的专家系统
常用于电子商务网站和媒体
简单的填充示例：根据共现关系推导相关性
基于协同过滤的个性化推荐

当模型接收到一个输入或触发某种事件时，状态会在有限的状态之间转换 = 有限状态的机器（有限状态机）
有限状态机又称有限自动机
可以用状态转换图表示

人机交互还是人机共生?
麦卡锡和恩格尔巴特
机器是会取代人类工人还是增强他们的能力？在某种层面上，这两种结果都会实现，但需要再次注意的是，这个问题本身就存在问题，它只会让我们得到偏颇的答案。
2010年春天，关于谷歌实验性汽车的传言在硅谷兴起，
2013年年底，已有超过6家汽车制造商公开表示计划推出自动驾驶汽车。2014年，商业化的沉寂终于被打破，包括宝马、奔驰、沃尔沃和奥迪在内的少数欧洲汽车制造商开始提供可选功能，比如堵车助手，这是走向自动驾驶的一小步。
维纳不仅对计算机革命做了早期的“黑暗”预言，还预见了一些更令人心惊胆战的事情：“……如果我们朝着制造机器的方向前进，这些机器可以学习，它们的行为可以通过经验来进行修正，那么我们必将面对这样一个事实：我们给予机器的任何程度的独立性都可能导致对我们自身意愿的反抗。瓶子里跑出的精灵不会心甘情愿地重新回到瓶子里，同样，我们也没有任何理由希望它们善待我们。”
1956年美国达特茅斯学院的夏季研讨会后，人工智能逐渐被视为新的研究领域，当时的约翰·麦卡锡还是达特茅斯学院一位年轻的数学教授。
麦卡锡认为“人工智能”一词与人类行为几乎毫无关系，它唯一可能暗示的是机器可以去执行类似人类执行的任务。
在人工智能历史的第一个10年，这种乐观无处不在，这从1956年达特茅斯学院的夏季研讨会上就可见一斑：
这项研究建立在一种猜想的基础之上，那就是学习的每一方面或智力的任何其他功能，原则上都可以准确地描述，并由机器模拟。我们将尝试，来寻找制造能够使用语言、提炼抽象概念的机器的方法，解决现在仍属于人类的各种问题，并完善人类自身。我们认为，如果一批优秀的科学家在一起研究一个夏天，那么这一领域中的一个或多个问题就能得到显著的推进。
不久之后，明斯基被麦卡锡的乐观情绪所带动，只留下一个研究生去处理机器视觉问题，因为这仅仅是一个夏季研究项目。“我们最终的目标是创造能够像人类一样高效地从经验中学习的程序。”麦卡锡写道。
计算能力的不断进步也让打造大规模神经网络、处理比以往更大的数据集成为可能。这一过程持续了近10年，不过那时，神经网络技术的发展、能量和价值都是毋庸置疑的。除计算机算力外，神经网络研究缺少的另一个关键成分便是用来训练网络的大型数据集。不过随着全球互联网的出现，这一局面很快就得到了改变——新的计算能力集中方式“云计算”逐渐浮出水面，并能够连接数十亿移动传感和计算系统——智能手机。现在，神经网络的训练变得越发简单，在网络上就能轻松获取数百万用于训练的图像和语音样本。
伴随着20世纪80年代人工智能的冬天的“滑铁卢”，在20世纪90年代，人工智能圈同样发生了显著的改变。它很大程度上放弃了最初形式化的、理性的、自上向下的约束——被称为GOFAI（Good Old-Fashioned Artificial Intelligence，有效的旧式人工智能），转而支持统计式的、“自底向上”，或者叫“建构主义者”的方法
20世纪80年代末，PARC计算机科学家马克·维瑟（Mark Weiser）曾预计，当微处理器成本、尺寸和功率大幅下降的时候，将计算机智能谨慎地整合入日常物品之中将成为可能。他将此称为“UbiComp”（ubiquitous computing，普适计算）。他认为，计算将可以“藏匿”在实物之中，就好像现在的电动马达、滑轮和皮带一样——成为一种无形的存在。
从长远来看，技术进步对大家都有益；但是如果缩短时间区间来看，并非所有人都会是赢家
